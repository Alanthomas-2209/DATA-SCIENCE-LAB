{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNreBQ6yH2elspFzCrxKoLU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# NLTK -NATURAL LANGUAGE TOOL KIT\n","\n","NLTK helps the computer to analysis, preprocess, and understand the written text.\n","\n","It contains a variety of libraries for various purposes like text classification, parsing, stemming, tokenizing, etc."],"metadata":{"id":"A351AYKsoWGi"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"tC-AQIlsmFYV","executionInfo":{"status":"ok","timestamp":1701757165968,"user_tz":-330,"elapsed":3108,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}}},"outputs":[],"source":["import nltk"]},{"cell_type":"markdown","source":["**punkt**\n","\n","In NLTK, PUNKT is an unsupervised trainable model, which means it can be trained on unlabeled data.\n","\n","It generates a list of sentences from a text by developing a model for words that start sentences, prepositional phrases, and abbreviations using an unsupervised technique. Without first being put to use, it has to be trained on a sizable amount of plaintext in the intended language\n","\n","The punkt module specifically focuses on sentence tokenization, dividing a piece of text into individual sentences based on the presence of punctuation marks like periods, question marks, and exclamation marks."],"metadata":{"id":"t94Lb7t1pGxC"}},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wFb4kikFpNCx","executionInfo":{"status":"ok","timestamp":1701757165968,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"f56ca78c-2bf3-41bb-abae-91ce71beb1ae"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["# PREPROCESSING STEPS IN TEXT ANALYTICS\n","\n","\n","Tokenization: Sentence segmentation, Word tokenization\n","\n","Stemming, Lemmatization\n","\n","Stop word Removal\n","\n","POS Tagging"],"metadata":{"id":"E7yjPsAl2sg_"}},{"cell_type":"markdown","source":["**Sentence Segmentation**"],"metadata":{"id":"xXpf94MamMvi"}},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize\n","\n","text=\"\"\"The BCCI on Wednesday extended the contracts of Indian cricket team head coach Rahul Dravid\n","        and his support staff following the side's runners-up finish in the ODI World Cup.\n","        Dravid had replaced Ravi Shastri after the T20 World Cup in 2021, getting appointed for a\n","        two-year term which ended with the ODI World Cup. Under Dravid, India also finished\n","        runners-up in the last World Test Championship, losing to Australia in the final.\"\"\"\n","\n","sentence_tokens=sent_tokenize(text)\n","print(sentence_tokens)\n","\n","for i in range(len(sentence_tokens)):\n","  print(\"\\n\",i+1,\" : \",sentence_tokens[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VgUji-XnmeLC","executionInfo":{"status":"ok","timestamp":1701757165968,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"3509b475-0359-4793-ea52-c390aea005ac"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[\"The BCCI on Wednesday extended the contracts of Indian cricket team head coach Rahul Dravid \\n        and his support staff following the side's runners-up finish in the ODI World Cup.\", 'Dravid had replaced Ravi Shastri after the T20 World Cup in 2021, getting appointed for a \\n        two-year term which ended with the ODI World Cup.', 'Under Dravid, India also finished \\n        runners-up in the last World Test Championship, losing to Australia in the final.']\n","\n"," 1  :  The BCCI on Wednesday extended the contracts of Indian cricket team head coach Rahul Dravid \n","        and his support staff following the side's runners-up finish in the ODI World Cup.\n","\n"," 2  :  Dravid had replaced Ravi Shastri after the T20 World Cup in 2021, getting appointed for a \n","        two-year term which ended with the ODI World Cup.\n","\n"," 3  :  Under Dravid, India also finished \n","        runners-up in the last World Test Championship, losing to Australia in the final.\n"]}]},{"cell_type":"code","source":["# text_hin=\"मलयालं, भाषा और लिपि के विचार से तमिल भाषा के काफी निकट है. इस पर संस्कृत का प्रभाव ईसा के पूर्व पहली सदी से हुआ है। संस्कृत शब्दों को मलयालम शैली के अनुकूल बनाने के लिए संस्कृत से अवतरित शब्दों को संशोधित किया गया है. अरबों के साथ सदियों से व्यापार संबंध अंग्रेजी तथा पुर्तगाली उपनिवेशवाद का असर भी भाषा पर पड़ा है.\"\n","# tokenized_text_hin=sent_tokenize(text_hin)\n","# print(tokenized_text_hin)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q09dHbAH3KY-","executionInfo":{"status":"ok","timestamp":1701681128697,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"af0030ab-8017-4aa9-a8d1-0755d42e6950"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['मलयालं, भाषा और लिपि के विचार से तमिल भाषा के काफी निकट है.', 'इस पर संस्कृत का प्रभाव ईसा के पूर्व पहली सदी से हुआ है। संस्कृत शब्दों को मलयालम शैली के अनुकूल बनाने के लिए संस्कृत से अवतरित शब्दों को संशोधित किया गया है.', 'अरबों के साथ सदियों से व्यापार संबंध अंग्रेजी तथा पुर्तगाली उपनिवेशवाद का असर भी भाषा पर पड़ा है.']\n"]}]},{"cell_type":"markdown","source":["**Word Tokenization**"],"metadata":{"id":"Gu0-V_TQrwkR"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","\n","text=\"\"\"The BCCI on Wednesday extended the contracts of Indian cricket team head coach Rahul Dravid and his support staff following the side's runners-up finish in the ODI World Cup. Dravid had replaced Ravi Shastri after the T20 World Cup in 2021, getting appointed for a two-year term which ended with the ODI World Cup. Under Dravid, India also finished runners-up in the last World Test Championship, losing to Australia in the final.\"\"\"\n","\n","word_tokens=word_tokenize(text)\n","print(word_tokens)\n","\n","for i in range(len(word_tokens)):\n","  print(\" \",i+1,\" : \",word_tokens[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j7OQk9Bfr0xQ","executionInfo":{"status":"ok","timestamp":1701757614728,"user_tz":-330,"elapsed":520,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"9a67359b-cbbc-4e46-8612-9d0a6d62ffe4"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["['The', 'BCCI', 'on', 'Wednesday', 'extended', 'the', 'contracts', 'of', 'Indian', 'cricket', 'team', 'head', 'coach', 'Rahul', 'Dravid', 'and', 'his', 'support', 'staff', 'following', 'the', 'side', \"'s\", 'runners-up', 'finish', 'in', 'the', 'ODI', 'World', 'Cup', '.', 'Dravid', 'had', 'replaced', 'Ravi', 'Shastri', 'after', 'the', 'T20', 'World', 'Cup', 'in', '2021', ',', 'getting', 'appointed', 'for', 'a', 'two-year', 'term', 'which', 'ended', 'with', 'the', 'ODI', 'World', 'Cup', '.', 'Under', 'Dravid', ',', 'India', 'also', 'finished', 'runners-up', 'in', 'the', 'last', 'World', 'Test', 'Championship', ',', 'losing', 'to', 'Australia', 'in', 'the', 'final', '.']\n","  1  :  The\n","  2  :  BCCI\n","  3  :  on\n","  4  :  Wednesday\n","  5  :  extended\n","  6  :  the\n","  7  :  contracts\n","  8  :  of\n","  9  :  Indian\n","  10  :  cricket\n","  11  :  team\n","  12  :  head\n","  13  :  coach\n","  14  :  Rahul\n","  15  :  Dravid\n","  16  :  and\n","  17  :  his\n","  18  :  support\n","  19  :  staff\n","  20  :  following\n","  21  :  the\n","  22  :  side\n","  23  :  's\n","  24  :  runners-up\n","  25  :  finish\n","  26  :  in\n","  27  :  the\n","  28  :  ODI\n","  29  :  World\n","  30  :  Cup\n","  31  :  .\n","  32  :  Dravid\n","  33  :  had\n","  34  :  replaced\n","  35  :  Ravi\n","  36  :  Shastri\n","  37  :  after\n","  38  :  the\n","  39  :  T20\n","  40  :  World\n","  41  :  Cup\n","  42  :  in\n","  43  :  2021\n","  44  :  ,\n","  45  :  getting\n","  46  :  appointed\n","  47  :  for\n","  48  :  a\n","  49  :  two-year\n","  50  :  term\n","  51  :  which\n","  52  :  ended\n","  53  :  with\n","  54  :  the\n","  55  :  ODI\n","  56  :  World\n","  57  :  Cup\n","  58  :  .\n","  59  :  Under\n","  60  :  Dravid\n","  61  :  ,\n","  62  :  India\n","  63  :  also\n","  64  :  finished\n","  65  :  runners-up\n","  66  :  in\n","  67  :  the\n","  68  :  last\n","  69  :  World\n","  70  :  Test\n","  71  :  Championship\n","  72  :  ,\n","  73  :  losing\n","  74  :  to\n","  75  :  Australia\n","  76  :  in\n","  77  :  the\n","  78  :  final\n","  79  :  .\n"]}]},{"cell_type":"code","source":["# tokenized_word_hin=word_tokenize(text_hin)\n","# print(tokenized_word_hin)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OSV5QHdM3zI3","executionInfo":{"status":"ok","timestamp":1701681129376,"user_tz":-330,"elapsed":37,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"2af5b22a-c76c-49be-ec7c-f467bef97d1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['मलयालं', ',', 'भाषा', 'और', 'लिपि', 'के', 'विचार', 'से', 'तमिल', 'भाषा', 'के', 'काफी', 'निकट', 'है', '.', 'इस', 'पर', 'संस्कृत', 'का', 'प्रभाव', 'ईसा', 'के', 'पूर्व', 'पहली', 'सदी', 'से', 'हुआ', 'है।', 'संस्कृत', 'शब्दों', 'को', 'मलयालम', 'शैली', 'के', 'अनुकूल', 'बनाने', 'के', 'लिए', 'संस्कृत', 'से', 'अवतरित', 'शब्दों', 'को', 'संशोधित', 'किया', 'गया', 'है', '.', 'अरबों', 'के', 'साथ', 'सदियों', 'से', 'व्यापार', 'संबंध', 'अंग्रेजी', 'तथा', 'पुर्तगाली', 'उपनिवेशवाद', 'का', 'असर', 'भी', 'भाषा', 'पर', 'पड़ा', 'है', '.']\n"]}]},{"cell_type":"markdown","source":["**Stemming**\n","\n","Stemming is a process of linguistic normalization, which reduces words to their word root word or chops off the derivational affixes.\n","\n","Default Stemmer :-Porter Stemmer\n"],"metadata":{"id":"KLISy2oDsi1K"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","ps = PorterStemmer()\n","\n","# choose some words to be stemmed\n","words = [\"program\", \"programs\", \"programmer\", \"programmers\",\"programming\",\"Studies\",\"homophone\"]\n","\n","for w in words:\n","    print(w, \" : \", ps.stem(w))\n","\n","# for w in word_tokens:\n","#     print(w, \" : \", ps.stem(w))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1MSUEyaMsmgg","executionInfo":{"status":"ok","timestamp":1701757575981,"user_tz":-330,"elapsed":386,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"05ccf996-ef76-4542-a819-29eb7188b6cf"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["program  :  program\n","programs  :  program\n","programmer  :  programm\n","programmers  :  programm\n","programming  :  program\n","Studies  :  studi\n","homophone  :  homophon\n"]}]},{"cell_type":"markdown","source":["**Lemmatization**\n","\n","Lemmatization reduces words to their base word, which is linguistically correct lemmas.\n","\n","It transforms root word with the use of dictionary.\n","\n","Lemmatization is usually more sophisticated than stemming.\n","\n","Stemmer works on an individual word without knowledge of the context.\n","\n","**wordnet**\n","\n","Wordnet is a publicly available lexical database of over 200 languages that provides semantic relationships between its words\n","\n","nltk.download('wordnet')\n","\n","The NLTK Lemmatization method is based on WordNet’s built-in morph function.\n"],"metadata":{"id":"oZEiRiwIupif"}},{"cell_type":"code","source":["nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sp9G-A5BwTpZ","executionInfo":{"status":"ok","timestamp":1701757435275,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"018eb3e7-5524-46c8-9457-4b8a7ae374ad"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["\n","from nltk.stem import WordNetLemmatizer\n","\n","ls = WordNetLemmatizer()\n","\n","# choose some words to be stemmed\n","words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\",\"study\",\"Studies\",\"studying\",\"homophone\"]\n","for word in words:\n","    print(word + \" ---> \" + ls.lemmatize(word))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lotvCOVotKqn","executionInfo":{"status":"ok","timestamp":1701757439312,"user_tz":-330,"elapsed":2474,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"93a8f84d-5e50-43ce-88ce-e958c5ec6688"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["program ---> program\n","programs ---> program\n","programmer ---> programmer\n","programming ---> programming\n","programmers ---> programmer\n","study ---> study\n","Studies ---> Studies\n","studying ---> studying\n","homophone ---> homophone\n"]}]},{"cell_type":"markdown","source":["*Always convert your text to lowercase before performing any NLP task including lemmatizing.*"],"metadata":{"id":"9NZrfiwmxXfo"}},{"cell_type":"markdown","source":["**Stopwords**\n","\n","Stopwords considered as noise in the text.\n","\n","Text may contain stop words such as is, am, are, this, a, an, the, etc."],"metadata":{"id":"40qfilALxxIe"}},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XMxhaRMfzl4o","executionInfo":{"status":"ok","timestamp":1701757594438,"user_tz":-330,"elapsed":335,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"3572ce80-428a-41b5-bd92-a17554415f47"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","\n","\n","stop_words = set(stopwords.words('english'))\n","print(\"stopwords in english:\",\"\\n\",stop_words,\"\\n\")\n","\n","filtered_tokens = []\n","\n","for w in word_tokens:\n","    if w.lower() not in stop_words:\n","        filtered_tokens.append(w.lower())\n","\n","print(\"original word_tokens:\",\"\\n\",word_tokens)\n","print(\"\\nafter stopword removal:\",\"\\n\",filtered_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QSWes09sxwxG","executionInfo":{"status":"ok","timestamp":1701757622193,"user_tz":-330,"elapsed":619,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"8f4a552c-11e5-4701-d53d-2636f87da1a8"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["stopwords in english: \n"," {'am', \"couldn't\", 'having', 'same', \"you're\", 'over', 'mightn', \"shouldn't\", 'of', 'just', 'them', 'ours', 'what', 'about', 'not', \"needn't\", 'theirs', 'against', 'me', 'the', 'up', 'itself', 'both', 'i', 'an', 'it', \"hasn't\", 'until', 'shan', 'themselves', 'each', 'ain', 'isn', 'than', 'if', 'that', \"mightn't\", 'because', 'at', \"it's\", 'so', 'own', 'your', 'wouldn', 'are', 'd', \"hadn't\", \"isn't\", 'she', \"mustn't\", 'after', 'from', 'haven', 's', \"should've\", 'out', 'did', 'y', 'didn', 'shouldn', 'before', 'most', 'hers', 'more', 'yourself', 'there', 'for', 'her', 'they', 'should', \"haven't\", 're', 'how', 'whom', 'few', 'above', 'and', 'very', 'hadn', \"won't\", 'won', 'which', \"you'd\", 'their', 'we', 'then', \"aren't\", 'these', 'can', 'hasn', 'will', 'do', 'being', 'doing', 'where', 'once', 'needn', 'into', \"wasn't\", 'ourselves', 'off', 'he', 'been', 'has', 'my', 'weren', 'have', 'or', 'on', 'by', 'our', \"don't\", 'why', 'some', 'himself', 'when', 'as', 'aren', \"she's\", 'this', 'such', \"doesn't\", 'who', 'a', 'his', 'other', 'mustn', \"wouldn't\", 'herself', 'be', 'yours', 'nor', 'further', 'between', \"that'll\", 'him', 'was', 'all', 'doesn', 'is', 'm', 'had', \"didn't\", 'those', 'in', 'with', 't', 'don', 'now', 'here', \"shan't\", 'wasn', 'you', 'while', 'again', 'o', 'to', 'its', 'but', 'under', \"you'll\", 'only', 'myself', 'no', 'too', 'were', 'couldn', 'below', 'any', \"you've\", 'down', 'ma', 'during', 'does', \"weren't\", 'yourselves', 'll', 'through', 've'} \n","\n","original word_tokens: \n"," ['The', 'BCCI', 'on', 'Wednesday', 'extended', 'the', 'contracts', 'of', 'Indian', 'cricket', 'team', 'head', 'coach', 'Rahul', 'Dravid', 'and', 'his', 'support', 'staff', 'following', 'the', 'side', \"'s\", 'runners-up', 'finish', 'in', 'the', 'ODI', 'World', 'Cup', '.', 'Dravid', 'had', 'replaced', 'Ravi', 'Shastri', 'after', 'the', 'T20', 'World', 'Cup', 'in', '2021', ',', 'getting', 'appointed', 'for', 'a', 'two-year', 'term', 'which', 'ended', 'with', 'the', 'ODI', 'World', 'Cup', '.', 'Under', 'Dravid', ',', 'India', 'also', 'finished', 'runners-up', 'in', 'the', 'last', 'World', 'Test', 'Championship', ',', 'losing', 'to', 'Australia', 'in', 'the', 'final', '.']\n","\n","after stopword removal: \n"," ['bcci', 'wednesday', 'extended', 'contracts', 'indian', 'cricket', 'team', 'head', 'coach', 'rahul', 'dravid', 'support', 'staff', 'following', 'side', \"'s\", 'runners-up', 'finish', 'odi', 'world', 'cup', '.', 'dravid', 'replaced', 'ravi', 'shastri', 't20', 'world', 'cup', '2021', ',', 'getting', 'appointed', 'two-year', 'term', 'ended', 'odi', 'world', 'cup', '.', 'dravid', ',', 'india', 'also', 'finished', 'runners-up', 'last', 'world', 'test', 'championship', ',', 'losing', 'australia', 'final', '.']\n"]}]},{"cell_type":"markdown","source":["**POS Tagging**\n","\n","Process of marking up a word in a corpus to a corresponding part\n","of a speech tag .\n","\n","Parts of speech are also known as word classes or lexical categories\n","\n","**Tag set:**\n","\n","   Flat Tag set\n","   Hierarchical Tag set\n"],"metadata":{"id":"sA24bpB31Z6N"}},{"cell_type":"code","source":["nltk.download('averaged_perceptron_tagger')\n","# NLTK's averaged_perceptron_tagger utilizes the Penn Treebank Tagset for labeling words\n","# in text with their respective parts of speech"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k221ZKbl1l-N","executionInfo":{"status":"ok","timestamp":1701757811080,"user_tz":-330,"elapsed":406,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"ce9e54cb-0be5-4e11-999c-3692a7b4a73c"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["tagged = nltk.pos_tag(filtered_tokens)\n","print(tagged)\n","\n","from collections import Counter\n","counts = Counter( tag for word,  tag in tagged)\n","print(counts)\n","\n","nltk.help.upenn_tagset(\"NN\")\n","\n","nltk.help.upenn_tagset(\"VBD\")\n","\n","nltk.help.upenn_tagset(\"VBG\")\n","nltk.help.upenn_tagset(\"VBZ\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlcjCcQl1dJ4","executionInfo":{"status":"ok","timestamp":1701758583381,"user_tz":-330,"elapsed":363,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"00f5ab8b-79dd-4b38-e0a1-f50e18a0f16b"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["[('bcci', 'NN'), ('wednesday', 'NN'), ('extended', 'VBD'), ('contracts', 'NNS'), ('indian', 'JJ'), ('cricket', 'NN'), ('team', 'NN'), ('head', 'NN'), ('coach', 'NN'), ('rahul', 'NN'), ('dravid', 'JJ'), ('support', 'NN'), ('staff', 'NN'), ('following', 'VBG'), ('side', 'NN'), (\"'s\", 'POS'), ('runners-up', 'JJ'), ('finish', 'JJ'), ('odi', 'NN'), ('world', 'NN'), ('cup', 'NN'), ('.', '.'), ('dravid', 'NN'), ('replaced', 'VBD'), ('ravi', 'JJ'), ('shastri', 'NN'), ('t20', 'NN'), ('world', 'NN'), ('cup', 'NN'), ('2021', 'CD'), (',', ','), ('getting', 'VBG'), ('appointed', 'VBN'), ('two-year', 'JJ'), ('term', 'NN'), ('ended', 'VBN'), ('odi', 'RB'), ('world', 'NN'), ('cup', 'NN'), ('.', '.'), ('dravid', 'NN'), (',', ','), ('india', 'NN'), ('also', 'RB'), ('finished', 'VBD'), ('runners-up', 'NN'), ('last', 'JJ'), ('world', 'NN'), ('test', 'NN'), ('championship', 'NN'), (',', ','), ('losing', 'VBG'), ('australia', 'JJ'), ('final', 'JJ'), ('.', '.')]\n","Counter({'NN': 27, 'JJ': 9, 'VBD': 3, 'VBG': 3, '.': 3, ',': 3, 'VBN': 2, 'RB': 2, 'NNS': 1, 'POS': 1, 'CD': 1})\n","NN: noun, common, singular or mass\n","    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n","    investment slide humour falloff slick wind hyena override subhumanity\n","    machinist ...\n","VBD: verb, past tense\n","    dipped pleaded swiped regummed soaked tidied convened halted registered\n","    cushioned exacted snubbed strode aimed adopted belied figgered\n","    speculated wore appreciated contemplated ...\n","VBG: verb, present participle or gerund\n","    telegraphing stirring focusing angering judging stalling lactating\n","    hankerin' alleging veering capping approaching traveling besieging\n","    encrypting interrupting erasing wincing ...\n","VBZ: verb, present tense, 3rd person singular\n","    bases reconstructs marks mixes displeases seals carps weaves snatches\n","    slumps stretches authorizes smolders pictures emerges stockpiles\n","    seduces fizzes uses bolsters slaps speaks pleads ...\n"]}]},{"cell_type":"code","source":["nltk.download('universal_tagset')\n","from collections import Counter\n","tagged= nltk.pos_tag(filtered_tokens, tagset='universal')\n","print(tagged)\n","counts = Counter( tag for word,  tag in tagged)\n","print(counts)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMM-YqrIAapK","executionInfo":{"status":"ok","timestamp":1701758706240,"user_tz":-330,"elapsed":339,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"37a75f21-970d-49fd-f4d9-78893847ce6c"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[('bcci', 'NOUN'), ('wednesday', 'NOUN'), ('extended', 'VERB'), ('contracts', 'NOUN'), ('indian', 'ADJ'), ('cricket', 'NOUN'), ('team', 'NOUN'), ('head', 'NOUN'), ('coach', 'NOUN'), ('rahul', 'NOUN'), ('dravid', 'ADJ'), ('support', 'NOUN'), ('staff', 'NOUN'), ('following', 'VERB'), ('side', 'NOUN'), (\"'s\", 'PRT'), ('runners-up', 'ADJ'), ('finish', 'ADJ'), ('odi', 'NOUN'), ('world', 'NOUN'), ('cup', 'NOUN'), ('.', '.'), ('dravid', 'NOUN'), ('replaced', 'VERB'), ('ravi', 'ADJ'), ('shastri', 'NOUN'), ('t20', 'NOUN'), ('world', 'NOUN'), ('cup', 'NOUN'), ('2021', 'NUM'), (',', '.'), ('getting', 'VERB'), ('appointed', 'VERB'), ('two-year', 'ADJ'), ('term', 'NOUN'), ('ended', 'VERB'), ('odi', 'ADV'), ('world', 'NOUN'), ('cup', 'NOUN'), ('.', '.'), ('dravid', 'NOUN'), (',', '.'), ('india', 'NOUN'), ('also', 'ADV'), ('finished', 'VERB'), ('runners-up', 'NOUN'), ('last', 'ADJ'), ('world', 'NOUN'), ('test', 'NOUN'), ('championship', 'NOUN'), (',', '.'), ('losing', 'VERB'), ('australia', 'ADJ'), ('final', 'ADJ'), ('.', '.')]\n","Counter({'NOUN': 28, 'ADJ': 9, 'VERB': 8, '.': 6, 'ADV': 2, 'PRT': 1, 'NUM': 1})\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"]}]},{"cell_type":"markdown","source":["**N-grams**\n"],"metadata":{"id":"Sd7rrdjqLFP5"}},{"cell_type":"code","source":["# Bigram\n","bi_output = list(nltk.bigrams(word_tokens))\n","print(bi_output,\"\\n\")\n","# Trigram\n","tri_output = list(nltk.trigrams(word_tokens))\n","print(tri_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ONr4gkXeEoqT","executionInfo":{"status":"ok","timestamp":1701758708296,"user_tz":-330,"elapsed":362,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"e1e462ee-40d3-4fda-c379-a42d7125b84e"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'BCCI'), ('BCCI', 'on'), ('on', 'Wednesday'), ('Wednesday', 'extended'), ('extended', 'the'), ('the', 'contracts'), ('contracts', 'of'), ('of', 'Indian'), ('Indian', 'cricket'), ('cricket', 'team'), ('team', 'head'), ('head', 'coach'), ('coach', 'Rahul'), ('Rahul', 'Dravid'), ('Dravid', 'and'), ('and', 'his'), ('his', 'support'), ('support', 'staff'), ('staff', 'following'), ('following', 'the'), ('the', 'side'), ('side', \"'s\"), (\"'s\", 'runners-up'), ('runners-up', 'finish'), ('finish', 'in'), ('in', 'the'), ('the', 'ODI'), ('ODI', 'World'), ('World', 'Cup'), ('Cup', '.'), ('.', 'Dravid'), ('Dravid', 'had'), ('had', 'replaced'), ('replaced', 'Ravi'), ('Ravi', 'Shastri'), ('Shastri', 'after'), ('after', 'the'), ('the', 'T20'), ('T20', 'World'), ('World', 'Cup'), ('Cup', 'in'), ('in', '2021'), ('2021', ','), (',', 'getting'), ('getting', 'appointed'), ('appointed', 'for'), ('for', 'a'), ('a', 'two-year'), ('two-year', 'term'), ('term', 'which'), ('which', 'ended'), ('ended', 'with'), ('with', 'the'), ('the', 'ODI'), ('ODI', 'World'), ('World', 'Cup'), ('Cup', '.'), ('.', 'Under'), ('Under', 'Dravid'), ('Dravid', ','), (',', 'India'), ('India', 'also'), ('also', 'finished'), ('finished', 'runners-up'), ('runners-up', 'in'), ('in', 'the'), ('the', 'last'), ('last', 'World'), ('World', 'Test'), ('Test', 'Championship'), ('Championship', ','), (',', 'losing'), ('losing', 'to'), ('to', 'Australia'), ('Australia', 'in'), ('in', 'the'), ('the', 'final'), ('final', '.')] \n","\n","[('The', 'BCCI', 'on'), ('BCCI', 'on', 'Wednesday'), ('on', 'Wednesday', 'extended'), ('Wednesday', 'extended', 'the'), ('extended', 'the', 'contracts'), ('the', 'contracts', 'of'), ('contracts', 'of', 'Indian'), ('of', 'Indian', 'cricket'), ('Indian', 'cricket', 'team'), ('cricket', 'team', 'head'), ('team', 'head', 'coach'), ('head', 'coach', 'Rahul'), ('coach', 'Rahul', 'Dravid'), ('Rahul', 'Dravid', 'and'), ('Dravid', 'and', 'his'), ('and', 'his', 'support'), ('his', 'support', 'staff'), ('support', 'staff', 'following'), ('staff', 'following', 'the'), ('following', 'the', 'side'), ('the', 'side', \"'s\"), ('side', \"'s\", 'runners-up'), (\"'s\", 'runners-up', 'finish'), ('runners-up', 'finish', 'in'), ('finish', 'in', 'the'), ('in', 'the', 'ODI'), ('the', 'ODI', 'World'), ('ODI', 'World', 'Cup'), ('World', 'Cup', '.'), ('Cup', '.', 'Dravid'), ('.', 'Dravid', 'had'), ('Dravid', 'had', 'replaced'), ('had', 'replaced', 'Ravi'), ('replaced', 'Ravi', 'Shastri'), ('Ravi', 'Shastri', 'after'), ('Shastri', 'after', 'the'), ('after', 'the', 'T20'), ('the', 'T20', 'World'), ('T20', 'World', 'Cup'), ('World', 'Cup', 'in'), ('Cup', 'in', '2021'), ('in', '2021', ','), ('2021', ',', 'getting'), (',', 'getting', 'appointed'), ('getting', 'appointed', 'for'), ('appointed', 'for', 'a'), ('for', 'a', 'two-year'), ('a', 'two-year', 'term'), ('two-year', 'term', 'which'), ('term', 'which', 'ended'), ('which', 'ended', 'with'), ('ended', 'with', 'the'), ('with', 'the', 'ODI'), ('the', 'ODI', 'World'), ('ODI', 'World', 'Cup'), ('World', 'Cup', '.'), ('Cup', '.', 'Under'), ('.', 'Under', 'Dravid'), ('Under', 'Dravid', ','), ('Dravid', ',', 'India'), (',', 'India', 'also'), ('India', 'also', 'finished'), ('also', 'finished', 'runners-up'), ('finished', 'runners-up', 'in'), ('runners-up', 'in', 'the'), ('in', 'the', 'last'), ('the', 'last', 'World'), ('last', 'World', 'Test'), ('World', 'Test', 'Championship'), ('Test', 'Championship', ','), ('Championship', ',', 'losing'), (',', 'losing', 'to'), ('losing', 'to', 'Australia'), ('to', 'Australia', 'in'), ('Australia', 'in', 'the'), ('in', 'the', 'final'), ('the', 'final', '.')]\n"]}]},{"cell_type":"markdown","source":["**Chunking**"],"metadata":{"id":"A78RA5Z3LTs6"}},{"cell_type":"code","source":["!pip install svgling\n","# svgling is a Python library used for visualizing\n","# syntactic trees (parse trees) in a graphical format using SVG (Scalable Vector Graphics)."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_dnE9LNuch-","executionInfo":{"status":"ok","timestamp":1701758820371,"user_tz":-330,"elapsed":8591,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"62c62a48-ec80-4889-e61e-e2ed3ccea33d"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting svgling\n","  Downloading svgling-0.4.0-py3-none-any.whl (23 kB)\n","Collecting svgwrite (from svgling)\n","  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: svgwrite, svgling\n","Successfully installed svgling-0.4.0 svgwrite-1.4.3\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk import pos_tag, RegexpParser\n","from nltk.tokenize import word_tokenize\n","from nltk.tree import Tree\n","from IPython.display import display\n","\n","# Sample sentence\n","sentence = \"The quick brown fox jumps over the lazy dog\"\n","\n","# Tokenize the sentence\n","tokens = word_tokenize(sentence)\n","\n","# Perform POS tagging\n","tagged = pos_tag(tokens)\n","\n","# Define a chunking grammar\n","# Using grammar, we create a chunk parser with the help of RegexpParser and apply it to our sentence.\n","\n","grammar = \"NP: {<DT>?<JJ>*<NN>}\" # DT if it exists, JJ in whatever number,  one NN\n","\n","# grammar = r\"\"\"\n","#     NP: {<DT>?<JJ>*<NN>}  # chunk determiner, adjectives, and nouns\n","#     VP: {<VB.*><NP|PP|CLAUSE>+$}  # chunk verbs and their arguments\n","#     PP: {<IN><NP>}  # chunk prepositions and their objects\n","#     CLAUSE: {<NP><VP>}  # chunk NP, VP\"\"\"\n","\n","# Create a chunk parser\n","chunk_parser = RegexpParser(grammar)\n","\n","# Parse the tagged sentence\n","parsed_sentence = chunk_parser.parse(tagged)\n","\n","\n","print(\"\\n Chunks:\\n\",parsed_sentence)\n","\n","# Convert the tree to an nltk.Tree object\n","tree = Tree.fromstring(str(parsed_sentence))\n","\n","# Display the tree\n","display(tree)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"wFc3jhQfINXg","executionInfo":{"status":"ok","timestamp":1701758823226,"user_tz":-330,"elapsed":486,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"9bcd198d-8035-46f5-ebfe-7975cdba0bd1"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Chunks:\n"," (S\n","  (NP The/DT quick/JJ brown/NN)\n","  (NP fox/NN)\n","  jumps/VBZ\n","  over/IN\n","  (NP the/DT lazy/JJ dog/NN))\n"]},{"output_type":"display_data","data":{"text/plain":["Tree('S', [Tree('NP', ['The/DT', 'quick/JJ', 'brown/NN']), Tree('NP', ['fox/NN']), 'jumps/VBZ', 'over/IN', Tree('NP', ['the/DT', 'lazy/JJ', 'dog/NN'])])"],"image/svg+xml":"<svg baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,648.0,120.0\" width=\"648px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"34.5679%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"28.5714%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The/DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.2857%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"35.7143%\" x=\"28.5714%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">quick/JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.4286%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"35.7143%\" x=\"64.2857%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">brown/NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.1429%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.284%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.87654%\" x=\"34.5679%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">fox/NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"39.5062%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"13.5802%\" x=\"44.4444%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">jumps/VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.2346%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"11.1111%\" x=\"58.0247%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">over/IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.5802%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"30.8642%\" x=\"69.1358%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"32%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the/DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"16%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"36%\" x=\"32%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">lazy/JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"32%\" x=\"68%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">dog/NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"84%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.5679%\" y1=\"19.2px\" y2=\"48px\" /></svg>"},"metadata":{}}]},{"cell_type":"code","source":["grammar = r\"\"\"\n","    NP: {<DT>?<JJ>*<NN>}  # chunk determiner, adjectives, and nouns\n","    VP: {<VB.*><NP|PP>*}  # chunk verbs and their arguments\n","    PP: {<IN><NP>}  # chunk prepositions and their objects\n","    \"\"\"\n","# Create a chunk parser\n","chunk_parser = RegexpParser(grammar)\n","\n","# Parse the tagged sentence\n","parsed_sentence = chunk_parser.parse(tagged)\n","\n","print(\"\\n Chunks:\\n\",parsed_sentence)\n","\n","# Convert the tree to an nltk.Tree object\n","tree = Tree.fromstring(str(parsed_sentence))\n","# Display the tree\n","display(tree)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"id":"BCe6XQ4dL2DR","executionInfo":{"status":"ok","timestamp":1701759089205,"user_tz":-330,"elapsed":363,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"0595be4b-e713-44f4-9ae7-fea6b50dd976"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Chunks:\n"," (S\n","  (NP The/DT quick/JJ brown/NN)\n","  (NP fox/NN)\n","  (VP jumps/VBZ)\n","  (PP over/IN (NP the/DT lazy/JJ dog/NN)))\n"]},{"output_type":"display_data","data":{"text/plain":["Tree('S', [Tree('NP', ['The/DT', 'quick/JJ', 'brown/NN']), Tree('NP', ['fox/NN']), Tree('VP', ['jumps/VBZ']), Tree('PP', ['over/IN', Tree('NP', ['the/DT', 'lazy/JJ', 'dog/NN'])])])"],"image/svg+xml":"<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,648.0,168.0\" width=\"648px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"34.5679%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"28.5714%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The/DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.2857%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"35.7143%\" x=\"28.5714%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">quick/JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.4286%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"35.7143%\" x=\"64.2857%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">brown/NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.1429%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.284%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.87654%\" x=\"34.5679%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">fox/NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"39.5062%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"13.5802%\" x=\"44.4444%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">jumps/VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.2346%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"41.9753%\" x=\"58.0247%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PP</text></svg><svg width=\"26.4706%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">over/IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"13.2353%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"73.5294%\" x=\"26.4706%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"32%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the/DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"16%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"36%\" x=\"32%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">lazy/JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"32%\" x=\"68%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">dog/NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"84%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.2353%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.0123%\" y1=\"19.2px\" y2=\"48px\" /></svg>"},"metadata":{}}]},{"cell_type":"markdown","source":["# NAMED ENTITY RECOGNITION"],"metadata":{"id":"B1BnxiMy5eHd"}},{"cell_type":"code","source":["nltk.download('maxent_ne_chunker')\n","nltk.download('words')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s1C1E3gW5z0P","executionInfo":{"status":"ok","timestamp":1701681129378,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"dd3f3801-7c2b-4539-e29a-881056f3a916"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":156}]},{"cell_type":"code","source":["from nltk import word_tokenize, pos_tag, ne_chunk\n","\n","sentence = \"Mark and John are working at Google.\"\n","\n","print (ne_chunk(pos_tag(word_tokenize(sentence))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMfPsMUC5cjl","executionInfo":{"status":"ok","timestamp":1701681129378,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"7727d7f7-aab8-4160-b68a-7810b8341f98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (PERSON Mark/NNP)\n","  and/CC\n","  (PERSON John/NNP)\n","  are/VBP\n","  working/VBG\n","  at/IN\n","  (ORGANIZATION Google/NNP)\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["**Smoothing**"],"metadata":{"id":"KqTrb8scOszy"}},{"cell_type":"code","source":["from collections import Counter\n","\n","# Sample corpus\n","corpus = [\n","    \"this is a sentence\",\n","    \"this sentence is another sentence\",\n","    \"yet another sentence\"\n","]\n","\n","# Tokenize the corpus\n","tokenized_corpus = [sentence.split() for sentence in corpus]\n","print(\"sentence list:\",tokenized_corpus,\"\\n\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-RHmjqlOw8C","executionInfo":{"status":"ok","timestamp":1701761543981,"user_tz":-330,"elapsed":536,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"8340e489-209d-4b00-cce9-ff6865f4e6d8"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["sentence list: [['this', 'is', 'a', 'sentence'], ['this', 'sentence', 'is', 'another', 'sentence'], ['yet', 'another', 'sentence']] \n","\n"]}]},{"cell_type":"code","source":["\n","# Vocabulary size\n","#vocabulary_size = unique unigrams\n","vocabulary_size = len(set([word for sentence in tokenized_corpus for word in sentence]))\n","print(\"\\nvocabulary_size:\",vocabulary_size,\"\\n\")\n","\n","# Calculate unigram, bigram, and trigram counts\n","unigram_counts = Counter(word for sentence in tokenized_corpus for word in sentence)\n","bigram_counts = Counter([(word1, word2) for sentence in tokenized_corpus for word1, word2 in zip(sentence[:-1], sentence[1:])])\n","trigram_counts = Counter([(word1, word2, word3) for sentence in tokenized_corpus for word1, word2, word3 in zip(sentence[:-2], sentence[1:-1], sentence[2:])])\n","\n","print(\"unigram_counts:\",unigram_counts,\"\\n\")\n","print(\"bigram_counts:\",bigram_counts,\"\\n\")\n","print(\"trigram_counts:\",trigram_counts,\"\\n\")\n","\n","# Calculate bigram counts\n","\n","# bigrams = [\n","#     (word1, word2)                           # This is the structure of a bigram, a tuple of two consecutive words\n","#     for sentence in tokenized_corpus         # Loop through each tokenized sentence in the corpus\n","#     for word1, word2 in zip(sentence[:-1], sentence[1:])]\n","                        # sentence[:-1] creates a sequence of words in the sentence except for the last word,slices the list up to the last element),\n","                        # sentence[1:] creates a sequence starting from the second word to the end of the sentence.\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZPy2_oHPnCB","executionInfo":{"status":"ok","timestamp":1701761581853,"user_tz":-330,"elapsed":381,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"49a6a198-4181-423a-ccea-ea3f33217e99"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","vocabulary_size: 6 \n","\n","unigram_counts: Counter({'sentence': 4, 'this': 2, 'is': 2, 'another': 2, 'a': 1, 'yet': 1}) \n","\n","bigram_counts: Counter({('another', 'sentence'): 2, ('this', 'is'): 1, ('is', 'a'): 1, ('a', 'sentence'): 1, ('this', 'sentence'): 1, ('sentence', 'is'): 1, ('is', 'another'): 1, ('yet', 'another'): 1}) \n","\n","trigram_counts: Counter({('this', 'is', 'a'): 1, ('is', 'a', 'sentence'): 1, ('this', 'sentence', 'is'): 1, ('sentence', 'is', 'another'): 1, ('is', 'another', 'sentence'): 1, ('yet', 'another', 'sentence'): 1}) \n","\n"]}]},{"cell_type":"code","source":["\n","# Function to calculate probability of uni-grams\n","def calculate_unigram_prob(word):\n","    count = unigram_counts[word]\n","    # print(count)\n","    total_words = sum(unigram_counts.values())\n","    # print(total_words)\n","    return count / total_words if total_words > 0 else 0.0\n","\n","\n","\n","# Function to calculate probability of bi-grams\n","def calculate_bigram_prob(bigram, count):\n","    starting_word_count = unigram_counts[bigram[0]]  # Count of the starting word in the bigram\n","    return count / starting_word_count if starting_word_count > 0 else 0.0\n","\n","\n","# Function to calculate probability of tri-grams\n","def calculate_trigram_prob(trigram, count):\n","    starting_bigram_count = bigram_counts[(trigram[0], trigram[1])]  # Count of the starting bigram in the trigram\n","    return count / starting_bigram_count if starting_bigram_count > 0 else 0.0\n","\n","# Calculate and print probabilities for specific n-grams\n","print(\"Unigram probabilities:\")\n","for unigram in unigram_counts:\n","    prob = calculate_unigram_prob(unigram)\n","    # prob =  calculate_ngram_prob(unigram, unigram_counts, vocabulary_size)\n","    print(f\"{unigram}: {prob:.4f}\")\n","\n","print(\"\\nBigram probabilities:\")\n","for bigram, count in bigram_counts.items():\n","    prob=calculate_bigram_prob(bigram,count)\n","    print(f\"{bigram}: {prob:.4f}\")\n","\n","print(\"\\nTrigram probabilities:\")\n","# for trigram in trigram_counts:\n","#     prob = calculate_ngram_prob(trigram, trigram_counts, vocabulary_size)\n","#     print(f\"{trigram}: {prob:.4f}\")\n","for trigram, count in trigram_counts.items():\n","  prob=calculate_trigram_prob(trigram,count)\n","  print(f\"{trigram}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5FfhpZyy5POv","executionInfo":{"status":"ok","timestamp":1701761588805,"user_tz":-330,"elapsed":431,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"fce1eaca-f997-4c9f-d01b-663db978cc95"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Unigram probabilities:\n","this: 0.1667\n","is: 0.1667\n","a: 0.0833\n","sentence: 0.3333\n","another: 0.1667\n","yet: 0.0833\n","\n","Bigram probabilities:\n","('this', 'is'): 0.5000\n","('is', 'a'): 0.5000\n","('a', 'sentence'): 1.0000\n","('this', 'sentence'): 0.5000\n","('sentence', 'is'): 0.2500\n","('is', 'another'): 0.5000\n","('another', 'sentence'): 1.0000\n","('yet', 'another'): 1.0000\n","\n","Trigram probabilities:\n","('this', 'is', 'a'): 1.0000\n","('is', 'a', 'sentence'): 1.0000\n","('this', 'sentence', 'is'): 1.0000\n","('sentence', 'is', 'another'): 1.0000\n","('is', 'another', 'sentence'): 1.0000\n","('yet', 'another', 'sentence'): 1.0000\n"]}]},{"cell_type":"code","source":["# Unseen test data containing the unseen bigram\n","test_sentence = \"this is a test sentence\"\n","\n","\n","tokenized_test = test_sentence.split()\n","\n","# Apply add-one (Laplace) smoothing for bigram probabilities\n","vocabulary_size = len(unigram_counts)\n","\n"],"metadata":{"id":"6ZfDNXH9w35F","executionInfo":{"status":"ok","timestamp":1701761605440,"user_tz":-330,"elapsed":429,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["\n","# Calculate smoothed probability for the unseen test bigram\n","test_bigram = (tokenized_test[2], tokenized_test[3])  # Extracting the 'sentence', 'is' bigram from the test data\n","test_starting_word_count = unigram_counts[test_bigram[0]]  # Count of the starting word in the test bigram\n","test_prob=(bigram_counts[test_bigram] ) / (test_starting_word_count)\n","test_smoothed_prob = (bigram_counts[test_bigram] + 1) / (test_starting_word_count + vocabulary_size)\n","\n","# Print original and smoothed bigram probabilities\n","print(\"Original Bigram Probabilities:\")\n","for bigram, count in bigram_counts.items():\n","    starting_word_count = unigram_counts[bigram[0]]\n","    prob = count / starting_word_count if starting_word_count > 0 else 0.0\n","    print(f\"Bigram {bigram}: Probability {prob:.4f}\")\n","\n","print(\"\\n Probability for Unseen Test Bigram:\")\n","print(f\"Bigram {test_bigram}: Probability {test_prob:.4f}\")\n","\n","print(\"\\nSmoothed Bigram Probabilities (with Add-One Smoothing):\")\n","smoothed_bigram_probs = {}\n","for bigram, count in bigram_counts.items():\n","    starting_word_count = unigram_counts[bigram[0]]  # Count of the starting word in the bigram\n","    smoothed_prob = (count + 1) / (starting_word_count + vocabulary_size)\n","    smoothed_bigram_probs[bigram] = smoothed_prob\n","\n","for bigram, prob in smoothed_bigram_probs.items():\n","    print(f\"Bigram {bigram}: Probability {prob:.4f}\")\n","\n","print(\"\\nSmoothed Probability for Unseen Test Bigram:\")\n","print(f\"Bigram {test_bigram}: Probability {test_smoothed_prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEMNI_ha36rl","executionInfo":{"status":"ok","timestamp":1701761611187,"user_tz":-330,"elapsed":358,"user":{"displayName":"Sree S Bhagya 212011","userId":"06342210365886464006"}},"outputId":"2b9259f3-4da4-4cbd-d066-9650f06ed06d"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Bigram Probabilities:\n","Bigram ('this', 'is'): Probability 0.5000\n","Bigram ('is', 'a'): Probability 0.5000\n","Bigram ('a', 'sentence'): Probability 1.0000\n","Bigram ('this', 'sentence'): Probability 0.5000\n","Bigram ('sentence', 'is'): Probability 0.2500\n","Bigram ('is', 'another'): Probability 0.5000\n","Bigram ('another', 'sentence'): Probability 1.0000\n","Bigram ('yet', 'another'): Probability 1.0000\n","\n"," Probability for Unseen Test Bigram:\n","Bigram ('a', 'test'): Probability 0.0000\n","\n","Smoothed Bigram Probabilities (with Add-One Smoothing):\n","Bigram ('this', 'is'): Probability 0.2500\n","Bigram ('is', 'a'): Probability 0.2500\n","Bigram ('a', 'sentence'): Probability 0.2857\n","Bigram ('this', 'sentence'): Probability 0.2500\n","Bigram ('sentence', 'is'): Probability 0.2000\n","Bigram ('is', 'another'): Probability 0.2500\n","Bigram ('another', 'sentence'): Probability 0.3750\n","Bigram ('yet', 'another'): Probability 0.2857\n","\n","Smoothed Probability for Unseen Test Bigram:\n","Bigram ('a', 'test'): Probability 0.1429\n"]}]}]}